META_ARC: "siamcar_r50"

BACKBONE:
    TYPE: "resnet50"
    KWARGS:
        used_layers: [2, 3, 4]
    PRETRAINED: '/home/xiancong/Project_all/SiamCART_self/pretrained_models/resnet50.model'  #/home/xiancong/Project_all/SiamCART_self/pretrained_models/resnet50.model
    TRAIN_LAYERS: ['layer2','layer3','layer4']
    TRAIN_EPOCH: 10 #10
    LAYERS_LR: 0.1

ADJUST:
    ADJUST: true
    TYPE: "AdjustAllLayer"
    KWARGS:
        in_channels: [512, 1024, 2048]
        out_channels: [256, 256, 256]


TRACK:
    TYPE: 'SiamCARTracker'
    PENALTY_K: 0.04
    WINDOW_INFLUENCE: 0.44
    LR: 0.33
    EXEMPLAR_SIZE: 127
    INSTANCE_SIZE: 255
    CONTEXT_AMOUNT: 0.5
    STRIDE: 8

TRAIN:
    EPOCH: 6
    START_EPOCH: 0
    BATCH_SIZE: 32 # 32
    BASE_LR: 0.005
    CLS_WEIGHT: 1.0
    LOC_WEIGHT: 3.0
    CEN_WEIGHT: 1.0
    RESUME: ''
    NUM_CLASSES: 2
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    OUTPUT_SIZE: 25 # 31

    LR:
        TYPE: 'log'
        KWARGS:
            start_lr: 0.005
            end_lr: 0.0005
    LR_WARMUP:
        TYPE: 'step'
        EPOCH: 5
        KWARGS:
            start_lr: 0.001
            end_lr: 0.005
            step: 1
#让模型先热热身，熟悉熟悉数据，学习率要小，相当于只是去看看，还没正式训练呢，学习率太高说不定就学跑偏了。
#在训练深度学习模型时，有时候需要先使用一个较小的学习率来进行预热，以避免在初始阶段出现不稳定的梯度或损失的情况。
#warmup_epochs就是控制预热的epoch数，也就是在训练的前几个epoch使用较小的学习率，使得模型能够更快地收敛到稳定的状态。
DATASET:
    NAMES: 
    - 'RGBT234'
#    - 'YOUTUBEBB'
#    - 'COCO'
#    - 'DET'

    TEMPLATE:
        SHIFT: 4
        SCALE: 0.05
        BLUR: 0.0
        FLIP: 0.0
        COLOR: 1.0

    SEARCH:
        SHIFT: 64
        SCALE: 0.18
        BLUR: 0.2
        FLIP: 0.0
        COLOR: 1.0

    NEG: 0.0
    GRAY: 0.0
